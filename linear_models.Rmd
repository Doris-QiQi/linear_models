---
title: "linear_models"
author: "Jingyi Yao"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

set.seed(1)

```

\ \par


## Model Fitting

  * interactions between variables can be specified using `*`.
  * intercept-only model `outcome ~ 1`
  * a model with no intercept `outcome ~ 0 + preidctors`
  * model using all available predictors `outcome ~ .` using the dot `.` to represent variables
  
```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% 
  select(price, stars, borough, neighborhood, room_type)

nyc_airbnb
```

\ \par
### 1. regression with categorical predictors(covariates)
  
R will treat **categorical covariates** appropriately and predictably: 

**indicator variables** are created for each **non-reference** category and included in your model, and the factor level is treated as the reference.

##### `borough` is a categorical predictor

##### the `intercept` term is the **reference group** -- `Bronx` in this example
```{r}
fit = lm(price ~ stars + borough, data = nyc_airbnb)
fit
```

\ \par
### 2. reorder factors: change the categorical covariate order/ change reference group

#### using `fct_infreq()` reorder the categories by frequency
#### the refence group is `Manhattan` now.

changing reference categories won’t change “fit” or statistical significance, but can affect ease of interpretation.

```{r}
nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(
    borough = fct_infreq(borough),      # reorder factor levels by frequency
    room_type = fct_infreq(room_type))


fit = lm(price ~ stars + borough, data = nyc_airbnb)

fit

```

\ \par
\ \par

## Tidying output

### 1. Type of `fit`

#### The output of a `lm` is an object of **class lm** which is a **list**

#### It is not a df but that can be manipulated using other functions. 
```{r}
fit

typeof(fit)

```

\ \par
### 2. The type of `summary(fit)`

#### summary produces an object of **class summary.lm**, which is also a **list**
```{r}
summary(fit)
typeof(summary(fit))

```

\ \par
### 3. `coef()` and `fitted.values()`function's output is a **vector**

`coef` produces a vector of coefficient values

`fitted.values` is a vector of fitted values.
```{r,include=FALSE}
coef(fit)

fitted.values(fit)

```


\ \par
### 4. `broom` package can tidy up the `fit` result

The broom package has functions for obtaining a quick summary of the model and for cleaning up the coefficient table.


\ \par
### 5. `broom::glance` produce a tibble of statistics
#### select certain terms(columns) from the tibble (df)
```{r}
fit %>% 
  broom::glance()

fit %>% 
  broom::glance() %>% 
  select(AIC)           # select a column    

```


\ \par
### 6. `broom::tidy` produce a tibble for each term in the regression

broom::tidy works with lots of things, including most of the functions for model fitting you’re likely to run into (survival, mixed models, additive models, …).
```{r}
fit %>% 
  broom::tidy()

```


\ \par
### 7.`str_replace(string, pattern, replacement)`

#### change the charater string format when they have a uniform format

#### pattern is the shared part in the strings

#### replacement is what you want to fill in to replace the previous pattern

```{r}
fit %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  mutate(term = str_replace(term, "borough", "Borough: ")) %>% 
  knitr::kable(digits = 3)


```


\ \par
\ \par
## Diagnostics

The `modelr` package can be used to **add residuals** and **fitted values** to a df.

### 1. `add_residuals(data,fit)` 
#### arguments are the original data and the fitted model
#### add a column named `resid`
```{r}
modelr::add_residuals(nyc_airbnb, fit)

```


\ \par
### 2. `add_predictions(data,fit)` 
#### arguments are the original data and the fitted model
#### add a column named `pred`
```{r}
modelr::add_predictions(nyc_airbnb, fit)

```


\ \par
### 3. plot : covariate vs. residuals
#### ggplot()+geom()+ ylim(lowerbound,upperbound) add a y limit
```{r}
nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = borough, y = resid)) + geom_violin() +
  ylim(-250,250)

```

\ \par
```{r}
nyc_airbnb %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = stars, y = resid)) + geom_point()
```


the presence of extremely large outliers in price and a generally skewed residual distribution. There are a few things we might try to do here – including creating a formal rule for the exclusion of outliers, transforming the price variable (e.g. using a log transformation), or fitting a model that is robust to outliers. 



\ \par
\ \par
## Hypothesis testing

### 1. compare the nested models

#### null hypothesis : the smaller model is better
#### alternative hypothesis : the bigger model is better
```{r}
fit_null = lm(price ~ stars + borough, data = nyc_airbnb)             # smaller model
fit_alt = lm(price ~ stars + borough + room_type, data = nyc_airbnb)  # bigger model

```


\ \par
### 2. use `anova(small,big)` to test the models

#### that this works for **nested** models only. 
#### Comparing **non-nested** models is a common problem that requires other methods;
#### **cross validation** may be helpful
```{r}
anova(fit_null, fit_alt) %>% 
  broom::tidy()

```




\ \par
\ \par

## Nesting data

#### use nest to create a list column containing datasets and fit separate models to each
#### nest within boroughs and fit borough-specific models 

\ \par
### 1. `nest(data = - category column)` 

#### get a df for each category (df under each category)

#### each category is now a list column that can be the argument for map()

\ \par
### 2. nest boroughs in NYC
#### `unest(results)` release the list columns for each category

```{r}
nest_lm_res =
  nyc_airbnb %>% 
  nest(df = -borough) %>%   # combine the columns except borough as a list
  mutate(
    models = map(.x = df, ~lm(price ~ stars + room_type, data = .x)), 
    results = map(models, broom::tidy)) %>%  # mao(listcol)
  select(-df, -models) %>%  # eliminate some tool columns only useful during the process
  unnest(results)   

nest_lm_res 

```


\ \par
### 3. `fct_inorder(term)` reorder the factors by the order in which they first appear

#### pivot_wider to change the model of each borough into one line

```{r}
nest_lm_res %>% 
  select(borough, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, values_from = estimate) %>%  # longer changed into wider
  knitr::kable(digits = 3)

```

### 4. nesting categories = using interactions(categorical variables)

nesting the boroughs = interact borough * room_type + borough * stars
```{r}
nyc_airbnb %>% 
  lm(price ~ stars * borough + room_type * borough, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

```



\ \par

### 5. **Stratified Analysis** : nest neighborhoods in Manhattan

#### stratified models make it easy to interpret covariate effects in each stratum,

#### does not provide a mechanism for assessing the significance of differences across strata

```{r}
manhattan_airbnb =
  nyc_airbnb %>% 
  filter(borough == "Manhattan")

manhattan_nest_lm_res =
  manhattan_airbnb %>% 
  nest(data = -neighborhood) %>% 
  mutate(
    models = map(data, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results)

manhattan_nest_lm_res 

```


```{r}
manhattan_nest_lm_res %>% 
  filter(str_detect(term, "room_type")) %>% 
  ggplot(aes(x = neighborhood, y = estimate)) + 
  geom_point() + 
  facet_wrap(~term) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))

```



\ \par
\ \par
## Mixed Models

#### when the factor has too many levels, it is not ideal to interact each level

#### try a mixed model : with random intercepts and slopes for each neighborhood


```{r}
manhattan_airbnb %>% 
  lme4::lmer(price ~ stars + room_type + (1 + room_type | neighborhood), data = .) %>% 
  broom.mixed::tidy()

```

\ \par
## Binary outcomes
\ \par
### 1. create a binary outcome `resolved`
```{r}
baltimore_df = 
  read_csv("data/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(resolved, victim_age, victim_race, victim_sex)


```


```{r}
f <- factor(c("a", "b", "c", "d"), levels = c("b", "c", "d", "a"))
fct_relevel(f)
fct_relevel(f, "a")
fct_relevel(f, "b", "a")
# Move to the third position
fct_relevel(f, "a", after = 2)

```

\ \par
### 2. use `glm(.,family = binomial())` to fit logistic regression
```{r}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 

summary(fit_logistic)

```

\ \par
### 3. logistic model estimates are **log odds ratios**
```{r}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%   # transform the log estimate into estimate
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)

```


\ \par
### 4. `modelr::add_predictions(fitted model)` add prediction to the original df

#### the prediction is the log odds ratio

### 5. inverse logit -- calculate the prob from the log odds ratio (pred)
```{r}
baltimore_df %>% 
  modelr::add_predictions(fit_logistic) %>% 
  mutate(fitted_prob = boot::inv.logit(pred))

```

