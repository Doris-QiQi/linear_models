---
title: "Bootstrapping"
author: "Jingyi Yao"
date: "`r Sys.Date()`"
output: github_document
---

## Introduction to Bootstrap

Bootstrapping is a popular resampling-based approach to statistical inference, and is helpful when usual statistical methods are intractable or inappropriate. The idea is to **draw repeated samples** from your original sample with **replacement**, thereby **approximating** the repeated sampling framework. Using list columns to store bootstrap samples is natural and provides a “tidy” approach to resampling-based inference.


Traditionally, the **distribution of a sample statistic** (sample mean, SLR coefficients, etc.) for repeated, random draws from a population has been established theoretically. These **theoretical distributions** make some **assumptions** about the **underlying population** from which samples are drawn, or depend on **large sample sizes** for asymptotic results. (Central Limit Theorem)


In cases where the **assumptions aren’t met**, or **sample sizes aren’t large enough** for asymptotics to kick in, it is still necessary to make inferences using the sample statistic. In these cases, **drawing repeatedly from the original population** would be great – one could simply draw a lot of samples and look at the **empirical** (rather than theoretical) distribution. 


Repeated sampling can happen on a computer though. To bootstrap, one draws repeated samples (with the **same sample size**) from the original sample with replacement to mimic the process of drawing repeated samples from the population. The bootstrap samples will differ from the original sample, and the sample statistic of interest (sample mean, SLR coefficients, etc.) can be computed for each bootstrap sample. Looking at the distribution of the statistic across samples gives a sense of the uncertainty in the estimate.


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

set.seed(1)
```


## Bootstrapping in SLR

### 1. generate 2 samples

#### const sample has the same error for each y -- linear regression assumption

#### nonconst sample has different error for each y
```{r}
n_samp = 250

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

sim_df_nonconst = sim_df_const %>% 
  mutate(
  error = error * .75 * x,
  y = 2 + 3 * x + error
)

```


### 2. bind the 2 samples 

#### `bind_rows(.id = "new column", a = df1, b = df2)` a and b are in new column
```{r}
sim_df = 
  bind_rows(const = sim_df_const, nonconst = sim_df_nonconst, .id = "data_source") 

sim_df
```


### 3. plot the fitted line using `+ stat_smooth(method = "lm")`
```{r}
sim_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  facet_grid(~data_source) 

```


These datasets have roughly the same overall variance, but the left panel shows data with constant variance and the **right panel** shows data with **non-constant variance**. For this reason, ordinary least squares should provide reasonable estimates in both cases, but inference is standard inference approaches may only be justified for the data on the left.


### 4. compare the SLR model results
```{r}
lm(y ~ x, data = sim_df_const) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

```


```{r}
lm(y ~ x, data = sim_df_nonconst) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

```


standard errors for coefficient estimates are similar in both cases.



## Drawing one bootstrap sample

### 1. write a function to do bootstrap

#### the original df is the argument

#### the output is the sample from the original df draw with replacement
```{r}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}
```


### 2. draw the bootstrap result and fit the model using `lm`

#### set the **alpha parameter** to show the overlapping points
```{r}
boot_sample(sim_df_nonconst) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +  # set the alpha to show if points are overlapped
  stat_smooth(method = "lm")

```


n comparison with the original data, the bootstrap sample has the same characteristics but isn’t a perfect duplicate – some original data points **appear more than once**, others **don’t appear at all**.


## Drawing many bootstrap samples

### 1. create a df to store the 1000 bootstrapped samples

#### `data_frame(column1, column2)` to create a df

#### `rerun(n = times, boot_sample(original df))` bootstrap for n times

#### `rerun(times, function())` rerun the sample function on the same argument for n times, the output is **listcol**

```{r}
boot_straps = 
  data_frame(
    strap_number = 1:1000,
    strap_sample = rerun(1000, boot_sample(sim_df_nonconst))
  )

boot_straps

```

### 2. **arrange** the sample and **pull** them out of the listcol
```{r}
boot_straps %>% 
  filter(strap_number %in% 1:2) %>% 
  mutate(strap_sample = map(strap_sample, ~arrange(.x, x))) %>% 
  pull(strap_sample)

```

### 3. **unnest** the listcol before plotting
```{r}
boot_straps %>% 
  filter(strap_number %in% 1:3) %>% 
  unnest(strap_sample) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm", se = FALSE) +
  facet_grid(~strap_number) 

```


This shows some of the differences across bootstrap samples, and shows that the fitted regression lines aren’t the same for every bootstrap sample.


## Analyzing bootstrap samples

### 1. result is based on model

#### delete sample and model before return the result

#### the output of `map()` is **listcol** -- need to unnest() before the output
```{r}
bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(y ~ x, data = .x) ),
    results = map(models, broom::tidy)) %>%  # use broom::tidy for each listcol model
  select(-strap_sample, -models) %>%  #  model is just a tool column
  unnest(results) 

bootstrap_results
```


### 2. calculate the overall estimate standard deviation

#### overall : all the bootstrap samples
```{r}
bootstrap_results %>% 
  group_by(term) %>% 
  summarize(boot_se = sd(estimate)) %>% 
  knitr::kable(digits = 3)

```


### 2. calculate the overall estimate CI
```{r}
bootstrap_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025), 
    ci_upper = quantile(estimate, 0.975))

```


### 3. show the fitted lines for each bootstrap sample

#### `ggplot()` arguments are still x and y

#### `geom_line(aes(group = ), stat = "smooth", method = "lm", alpha = .1)`

#### group = unnested list_col, stat_smooth = "lm"
```{r}
boot_straps %>% 
  unnest(strap_sample) %>%   # first unnest()
  ggplot(aes(x = x, y = y)) + 
  geom_line(aes(group = strap_number), stat = "smooth", method = "lm", se = FALSE, alpha = .1, color = "blue") +
  geom_point(data = sim_df_nonconst, alpha = .5)

```



## Bootstrap in modelr

### 1. boostrap(n= ) the modelr function's argument is the number of samples

#### the output is a df with listcol
```{r}
boot_straps = 
  sim_df_nonconst %>% 
  modelr::bootstrap(n = 1000)

boot_straps

boot_straps$strap[[1]]

```

### 2. transform the listcol item into df
```{r}
as_data_frame(boot_straps$strap[[1]])

```


### calculate the sd of estimates
```{r}
sim_df_nonconst %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(y ~ x, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(boot_se = sd(estimate))

```


## Airbnb Example
```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% 
  drop_na(price, stars) %>% 
  select(price, stars, borough, neighborhood, room_type)

```


```{r}
nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price, color = room_type)) + 
  geom_point() 

```


some **large outliers in price** might affect **estimates** and **inference** for the association between star rating and price. Because estimates are likely to be sensitive to those outliers and “usual” rules for inference may not apply.



```{r}
nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~ lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(results) %>% 
  unnest(results) %>% 
  filter(term == "stars") %>% 
  ggplot(aes(x = estimate)) + geom_density()


```

This distribution has a heavy tail extending to low values and a bit of a “shoulder”, features that may be related to **the frequency with which large outliers are included in the bootstrap sample**.


The estimate distribution should be approximately normal according to the assumption. But the plot shows that it is indeed skewed. This is probably caused by the samples. The outliers may often appear in the chosen samples.






